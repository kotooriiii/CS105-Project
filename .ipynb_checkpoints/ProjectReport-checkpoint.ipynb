{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa5e059-dff5-4f94-815d-cb1d338728e9",
   "metadata": {},
   "source": [
    "# CS105 Final Project: Using Text Analysis on Movie Scripts to Predict Genre\n",
    "### By: Tran Nguyen, Nina Shenoy, Carlos Miranda, Kathleen Dario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717c346-9b47-4d43-b85f-50700da65a4f",
   "metadata": {},
   "source": [
    "Our project idea stemmed from our collective love of movies. After learning about different machine learning + classification techniques in class, we questioned whether we could apply these concepts to movie scripts, and see if we could correctly predict the genre of a movie based on the words in it's script. The techniques we will use are tf-idf, KNN, and cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebfe6c4e-d266-4d2e-8be1-8ec58ad0e03a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11784/1244246543.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteractiveshell\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInteractiveShell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "#Necessary imports for plotting, pandas, math operations, and multiple print statements.\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2, chi2_contingency\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import enchant\n",
    "enchant_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "#nltk.download('popular') download needed packagges\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "evenmore_stopwords_list = ['like', 'well', 'oh', 'like', 'yeah', 'he\\'s']\n",
    "useless_words_list = ['know', 'knew', 'got', 'get', 'gonna']\n",
    "stopwords_list_2 = ['go', 'right', 'come', 'good', 'okay', 'one', 'would', 'want', 'think', 'see', 'back', 'hey', 'i\\'m', 'i\\'ll', 'that\\'s','he\\'s', 'can\\'t', 'said', 'this', 'going']   \n",
    "stopwords_list = stopwords_list + evenmore_stopwords_list + useless_words_list + stopwords_list_2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaa9e0-4090-4b30-a9b0-c0cdc8eb574a",
   "metadata": {},
   "source": [
    "## 2. Data Collection & Data Cleaning\n",
    "#### (Carlos Miranda + Nina Shenoy + Tran Ngueyn + Kathleen Dario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57960849-e160-464c-8ed8-8ddbaaf71fc8",
   "metadata": {},
   "source": [
    "We retrieved our data from 2 sources: IMDb and Kaggle. The IMDB datasets were used to obtain information about the movies title, genre(s), release date, language, etc. while the Kaggle dataset was used to retrieve the movie scripts. We used inner joins to combine the 3 datasets together, using the IMDB ID as the joining factor.\n",
    "\n",
    "After combining our datasets, we then had to filter out any unneeded entries.  The IMDb datasets contained tens of thousands of movie entries, dating as far back to the 1800's. We decided that for this project, we will only use feature-length films (excludinv tv series, short-films, etc) that were released in the years (1960-2021) that are in English, and are from the regions of US and Canada. This signifigantly reduced the size of our original dataset, which made runtime faster and easier for us to analyze.\n",
    "\n",
    "We also noticed that some movies had more than one entry. This was because there were instances where two scripts would correspond to a particular movie. To resolve this, we got rid of the longer script, as it contained unnecessary data such as exposition, cues, and character names, and we mainly wanted to focus on dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61753664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDF(df):\n",
    "    df_copy = df.copy()\n",
    "    filtered_words_x = ['go', 'right', 'come', 'good']\n",
    "    df_copy['Scripts'] = df_copy['Scripts'].apply(lambda x: ' '.join([item for item in x.split() if item.lower() not in filtered_words_x]))\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04907228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCSV(scripts_basics_df, outputpath):\n",
    "    #cleaning data\n",
    "    scripts_basics_df = scripts_basics_df.dropna() #delete NaN values\n",
    "    scripts_basics_df = scripts_basics_df.astype({'Scripts':'string'})\n",
    "\n",
    "    scripts_basics_df[\"Scripts\"] = scripts_basics_df[\"Scripts\"].replace(to_replace=r\"<.*>\", value=\"\", regex=True)\n",
    "    scripts_basics_df[\"Scripts\"] = scripts_basics_df[\"Scripts\"].replace(to_replace=r\"[^a-zA-Z0-9\\'â€™\\ ]\", value=\"\", regex=True)\n",
    "    #scripts_basics_df['Scripts'] = scripts_basics_df['Scripts'].apply(lambda x: ' '.join([item for item in x.split() if not all([word in string.ascii_uppercase for word in item])]))\n",
    "    scripts_basics_df['Scripts'] = scripts_basics_df['Scripts'].apply(lambda x: ' '.join([item for item in x.split() if item.lower() not in stopwords_list]))\n",
    "    scripts_basics_df['Scripts'] = scripts_basics_df['Scripts'].apply(lambda x: ' '.join([item for item in x.split() if enchant_dict.check(item)]))\n",
    "    #scripts_basics_df['Scripts']\n",
    "    scripts_basics_df = scripts_basics_df[scripts_basics_df['Scripts'].str.split().str.len().ge(100)]\n",
    "    scripts_basics_df = scripts_basics_df.dropna() #delete NaN values\n",
    "    scripts_basics_df['Scripts'] = scripts_basics_df['Scripts'].str.lower()\n",
    "    scripts_basics_df['Scripts'] = scripts_basics_df['Scripts'].apply(lambda x: ' '.join([item for item in x.split() if item not in stopwords_list]))\n",
    "    scripts_basics_df = scripts_basics_df.loc[:, ~scripts_basics_df.columns.str.startswith('Unnamed')] \n",
    "    scripts_basics_df.to_csv(outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4447dd-66d2-4ba7-8173-92196f1ce388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicates(scripts_basics_df):\n",
    "    # Key = IMDB ID , Value = Size of script\n",
    "    smallestMovieScriptMap = {}\n",
    "    # Key = IMDB ID , Value = index\n",
    "    keepIndexMap = {}\n",
    "\n",
    "    #show all columns\n",
    "    scripts_basics_df.dtypes\n",
    "\n",
    "    scripts_basics_df['Scripts'] = scripts_basics_df['Scripts'].apply(str)\n",
    "\n",
    "    for index, row in scripts_basics_df.iterrows():\n",
    "        constID = row['IMDB ID']\n",
    "        currentSize = len(row['Scripts'])\n",
    "\n",
    "        if constID in smallestMovieScriptMap:\n",
    "            smallestSize = smallestMovieScriptMap[constID]\n",
    "            if currentSize < smallestSize:\n",
    "                smallestMovieScriptMap[constID] = currentSize\n",
    "                keepIndexMap[constID] = index\n",
    "        else:\n",
    "            smallestMovieScriptMap[constID] = currentSize\n",
    "            keepIndexMap[constID] = index\n",
    "    # keep this removed, this removes first instance\n",
    "    # scripts_basics_df = scripts_basics_df[~scripts_basics_df.index.duplicated(keep='first')]\n",
    "    return scripts_basics_df[scripts_basics_df.index.isin(keepIndexMap.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce143da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_basics_df = pd.read_csv(\"datasets/bettermovies.csv\")\n",
    "scripts_basics_df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86b984c-17a8-4d1c-b490-e5f9924303cd",
   "metadata": {},
   "source": [
    "Since some movies were classified as having multiple genres, we ended up splitting them into multiple entries in our data set, with each entry having one genre. This made it easier for us to calculate the frequencies of the different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbcdbb2-cd5a-4f99-a2fe-9ef3b99e8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts_basics_df[\"genres\"] = scripts_basics_df[\"genres\"].str.split(\",\")\n",
    "scripts_basics_df = scripts_basics_df.explode(\"genres\")\n",
    "#display(scripts_basics_df[[\"originalTitle\", \"Scripts\", \"genres\"]])\n",
    "#display(scripts_basics_df[\"genres\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0e90c-04ea-4174-b347-5dd3e549280d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "#### (Nina Shenoy + Kathleen Dario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf50f48-3c5a-40c1-9b5f-d7a0c2794400",
   "metadata": {},
   "source": [
    "Since our topic focuses heavily on genres and scripts, we wanted to see if we could find any relationships between these two variables. The three major things we focused on are:\n",
    "1. The most common film genres in our dataset.\n",
    "2. Which genre(s) have the most words in their scripts, and what the average script length is for that particular genre.\n",
    "3. What the most common words are in each genre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5865c2-c92c-445b-84dc-e1049e3e3862",
   "metadata": {},
   "source": [
    "### What are the most common film genres in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4134564",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_frequency = scripts_basics_df.genres.value_counts()\n",
    "print(genre_frequency)\n",
    "genre_frequency.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f343638-85e6-45dd-9c7c-d35c0435b484",
   "metadata": {},
   "source": [
    "As we can see per the above bar chart, the majority of the films in the dataset are classified as Drama, Comedy, and/or Action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67d57c-d738-4429-9f3b-40c9eba2af9d",
   "metadata": {},
   "source": [
    "### Which genre(s) have the most words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a09306-32f8-48e4-86c4-ebc8243d7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = scripts_basics_df[[\"originalTitle\", \"Scripts\", \"genres\"]]\n",
    "new_df = new_df.dropna() #can be deleted since data should be clean before this step, kept for testing\n",
    "new_df['Length'] = new_df['Scripts'].str.split(\" \").str.len() # adds the length column with length of each script\n",
    "#display(new_df)\n",
    "\n",
    "\n",
    "\n",
    "new_df.boxplot(column='Length',by='genres', vert=False)\n",
    "\n",
    "grouped = new_df.groupby(['genres']).median().sort_values(by='Length')\n",
    "\n",
    "plot_df = sns.boxplot(x=new_df.genres, y=new_df.Length, order=grouped.index)\n",
    "plot_df.set_xticklabels(plot_df.get_xticklabels(),rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502d5f0",
   "metadata": {},
   "source": [
    "Special Notes: Documentary has the highest average length but it is also worth to note that there is an extremely small sample size. Horror, in this case, appears to have the smallest script length possibly due to trying to put emphasis on the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d94a9-5edb-4932-96a1-e9e7a3b8d9bb",
   "metadata": {},
   "source": [
    "### What are the most common words in each genre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19dbf49-233a-40a1-85de-b9b8cd7b7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def cleanDF(df):\n",
    "    df_copy = df.copy()\n",
    "    filtered_words_x = ['go', 'right', 'come', 'good', 'okay', 'one', 'would', 'want', 'think', 'see', 'back', 'hey', 'i\\'m', 'i\\'ll', 'that\\'s','he\\'s', 'can\\'t', 'said', 'this']    \n",
    "    df_copy['Scripts'] = df_copy['Scripts'].apply(lambda x: ' '.join([item for item in x.split() if item.lower() not in filtered_words_x]))\n",
    "    return df_copy\n",
    "\n",
    "words_df = scripts_basics_df[[\"originalTitle\", \"Scripts\", \"genres\"]]\n",
    "words_df = words_df.dropna()\n",
    "words_df = words_df.astype({'Scripts':'string'})\n",
    "words_df = words_df.groupby(['genres']).agg({'Scripts': ' '.join})\n",
    "words_df\n",
    "\n",
    "clean_df = cleanDF(words_df)\n",
    "clean_df[\"Most Common\"] = pd.DataFrame({\"MostCommon\" : clean_df['Scripts'].str.split(\" \").apply(lambda x: [k for k, v in Counter(x).most_common(10)])})\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(clean_df[[\"Most Common\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "mask = np.array(Image.open(\"heart.png\"))\n",
    "\n",
    "def printCloud(genre):\n",
    "    wordcloud = WordCloud(width=1600, height=800).generate((clean_df.loc[genre, \"Scripts\"]))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4684ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_list_o = {'Comedy', 'Fantasy', 'Romance', 'Drama', 'Thriller', 'Action', 'Crime', 'Adventure', \n",
    "     'Family', 'Mystery', 'Biography', 'Music', 'Horror', 'Sci-Fi', 'Animation',  \n",
    "     'Sport', 'History',  'Western', 'War',  'Musical', 'Documentary'\n",
    "}\n",
    "\n",
    "#Get word cloud based on genre\n",
    "for genre in genres_list_o:\n",
    "    printCloud(genre)\n",
    "    break; #get out of loop only for pdf report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f29d62-31df-434f-9603-2311cf7d7df5",
   "metadata": {},
   "source": [
    "## Analysis + Techniques\n",
    "#### (Tran Nguyen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ba93dd-4197-4367-a2d7-820fb49669a9",
   "metadata": {},
   "source": [
    "### TF IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb64284-aa82-4c70-9675-a3a91bf91c32",
   "metadata": {},
   "source": [
    "After cleaning the dataset, we perform TF-IDF on the scripts to find the frequencies of each word in each script. The TF-IDF table above shows that there are a total of 58,420 unique words in the 2018 movie scripts we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b7b99b-be8f-4816-9f72-9c80c800a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cleaned dataset\n",
    "df = pd.read_csv(\"datasets/cleanest.csv\")\n",
    "\n",
    "# get only necessary columns \n",
    "main_df = df[[\"originalTitle\", \"Scripts\", \"genres\"]]\n",
    "main_df = main_df[main_df[\"Scripts\"].str.contains(\"<NA>\") == False].reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c27f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf idf \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "response = tfidf.fit_transform(main_df[\"Scripts\"])\n",
    "feature_names = tfidf.get_feature_names()\n",
    "vectorized_df = pd.DataFrame(response.toarray()).set_axis(feature_names, axis=1)\n",
    "vectorized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf753d28-88f1-4c21-a2e8-6b19a35d9276",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "\n",
    "For predicting the genres of our movies, we use K-Nearest Neighbor as our classifier. The classifying process is as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac791e5-b226-4780-bf72-22195c364dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "We first use the Train-Test Split technique to split the dataset into two sets: \n",
    "    - one for training, which contains the neighbors for KNN\n",
    "    - one for testing, which contains the new instances we want to add to our data\n",
    "1800 out of 2018 scripts, or 89% of the dataset, are used for training. The rest is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bce540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit vectorized dataframe\n",
    "vectorized_df.insert(0, \"movie_genres\", main_df[\"genres\"].copy())\n",
    "vectorized_df.insert(0, \"movie_titles\", main_df[\"originalTitle\"].copy())\n",
    "\n",
    "# divide into train and test\n",
    "vectorized_train = vectorized_df.loc[:1800].copy()\n",
    "vectorized_test = vectorized_df.loc[1801:].copy()\n",
    "vectorized_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92310a7-2918-4932-b5da-215962273a75",
   "metadata": {},
   "source": [
    "Looking at the above training set, we can see that movies can have multiple genres. Thus we want to separate the genres for training purposes. Any movie with multiple genres will have multiple instances in the training set, each instance with one genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006576f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting genres only for train\n",
    "vectorized_train[\"movie_genres\"] = vectorized_train[\"movie_genres\"].squeeze().str.split(\",\")\n",
    "vectorized_train = vectorized_train.explode(\"movie_genres\")\n",
    "vectorized_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795d4398-03ed-448e-97dd-219dd9a0c9e6",
   "metadata": {},
   "source": [
    "Now that each row has only one genre, we take a look at the distribution of our training set. Our dataset has a very skewed distribution: there are 1021 Drama movies while there are only 9 Western movies.\n",
    "Having even distribution of neighbors would help with our classifier's performance. Thus, we only select the top 7 genres that have enough data for our classification and discard the rest. For each of these 7 genres, 300 movies are selected for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222c6fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_train[\"movie_genres\"].value_counts()\n",
    "\n",
    "# get 250 movies for each genre (Drama, Comedy, Action, Crime, Romance, Adventure, Thriller)\n",
    "vectorized_train[\"movie_genres\"].value_counts()\n",
    "sorted_df = vectorized_train.sort_values(by=['movie_genres'], ascending=True)\n",
    "\n",
    "not_remove = {'Action', 'Adventure', 'Comedy', 'Crime', 'Drama',  'Romance', 'Thriller'}\n",
    "remove = {'Mystery', 'Fantasy', 'Sci-Fi', 'Biography', 'Animation', 'Family', 'Horror',  'Sport', 'Music', 'War', 'History', 'Musical', 'Western', 'Documentary'}\n",
    "\n",
    "sorted_df = sorted_df[~sorted_df[\"movie_genres\"].isin(remove)]\n",
    "\n",
    "vectorized_train = vectorized_train.iloc[0:0]\n",
    "for x in not_remove:\n",
    "    temp = sorted_df.loc[sorted_df[\"movie_genres\"] == x]\n",
    "    vectorized_train = vectorized_train.append(temp.iloc[:300])\n",
    "\n",
    "# get list of movies used for training (so we dont use them in testing)\n",
    "train_movies = list(set(vectorized_train[\"movie_titles\"].tolist()))\n",
    "vectorized_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83744209-1868-4adf-bb28-4a7f55b483c1",
   "metadata": {},
   "source": [
    "Currently, our genres are categorical variables. To perform KNN and make predictions, we must convert them into numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9db27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert \"genres\" from categorical to quantitative\n",
    "d = {'Action': 0, 'Adventure': 1, 'Comedy': 2, 'Crime': 3, 'Drama': 4,  'Romance': 5, 'Thriller': 6}\n",
    "\n",
    "vectorized_train[\"movie_genres\"] = vectorized_train[\"movie_genres\"].map(d)\n",
    "vectorized_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a8664-c23f-4fa7-b389-048b744470a1",
   "metadata": {},
   "source": [
    "For the purposes of evaluating our classifier's performance, we also separate the genres of the testing set and convert them into numerical values. For each movie, the original string of genres is converted into a list of numerical genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d72af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert genres in test set to numbers and put them into lists\n",
    "vectorized_test.head()\n",
    "\n",
    "# remove movies that are in train set from test set\n",
    "test_movies = list(set(vectorized_test[\"movie_titles\"].tolist()))\n",
    "for x in list(set(test_movies).intersection(train_movies)):\n",
    "    vectorized_test = vectorized_test[vectorized_test[\"movie_titles\"] != x]\n",
    "\n",
    "# split movie_genres into one genre per line\n",
    "vectorized_test_temp = vectorized_test.copy()\n",
    "vectorized_test_temp[\"movie_genres\"] = vectorized_test_temp[\"movie_genres\"].squeeze().str.split(\",\")\n",
    "vectorized_test_temp = vectorized_test_temp.explode(\"movie_genres\")\n",
    "\n",
    "# filter out movies from test set that don't have the genres we're keeping\n",
    "for x in remove:\n",
    "    vectorized_test = vectorized_test[vectorized_test[\"movie_genres\"] != x]\n",
    "    vectorized_test_temp = vectorized_test_temp[vectorized_test_temp[\"movie_genres\"] != x]\n",
    "    \n",
    "# convert genres to numbers and put them into lists\n",
    "vectorized_test_temp[\"movie_genres\"] = vectorized_test_temp[\"movie_genres\"].map(d)\n",
    "vectorized_test_temp = vectorized_test_temp.groupby(\"movie_titles\")[\"movie_genres\"].apply(list).reset_index()\n",
    "\n",
    "# change string genres to list of numerical genres\n",
    "vectorized_test = vectorized_test.sort_values(by=['movie_titles'], ascending=True)\n",
    "vectorized_test_temp = vectorized_test_temp.sort_values(by=['movie_titles'], ascending=True)\n",
    "vectorized_test[\"movie_genres\"] = vectorized_test_temp[\"movie_genres\"].values\n",
    "vectorized_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a28d2",
   "metadata": {},
   "source": [
    "### KNN using Jaccard Similarity and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c7e5b-9330-4b73-85c0-9edb110c1ce4",
   "metadata": {},
   "source": [
    "With our training and testing sets ready, we go ahead and perform our classification.\n",
    "- We choose the value k = sqrt(n), where n is the size of our training test\n",
    "- To calculate the distance of the new instance and its k neighbors, or in our context, the similarity of one script with its neighbors, we use two similarity measuring methods: Cosine Similarity and Jaccard similarity\n",
    "- We also use weighted distance method during our similarity calculation to aim for a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import math\n",
    "\n",
    "X_train = vectorized_train.copy().drop([\"movie_titles\", \"movie_genres\"], axis=1)\n",
    "y_train = vectorized_train[\"movie_genres\"]\n",
    "X_test = vectorized_test.copy().drop([\"movie_titles\", \"movie_genres\"], axis=1)\n",
    "y_test = vectorized_test[\"movie_genres\"]\n",
    "\n",
    "# cosine similarity calculation\n",
    "def cos_sim(x, y):\n",
    "    return dot(x, y) / (norm(x) * norm(y))\n",
    "\n",
    "k = int(math.sqrt(X_train.shape[0]))\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=k, metric=cos_sim)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k, metric=\"jaccard\")\n",
    "# knn_model = KNeighborsClassifier(n_neighbors=k, weights='distance', metric=\"jaccard\")\n",
    "\n",
    "knn_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418caa5e-6cd5-496f-a81f-aed06d8ae21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing predicted genre with actual genres\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "test_preds = knn_model.predict(X_test)\n",
    "compare = vectorized_test[[\"movie_titles\",\"movie_genres\"]]\n",
    "compare = compare.assign(predicted = pd.Series(test_preds).values)\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0266d895-ff7b-4baf-829b-4f85e7e198fb",
   "metadata": {},
   "source": [
    "After many tests, the method that gives our classifier the best performance is to use $k = sqrt(n)$ and Jaccard Similarity without weighted distances. This gives us an accurancy of 64% to predict the genre of the 200 movies in our testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of our model\n",
    "\n",
    "# compute the error percentage of two arrays\n",
    "# actual is an array of arrays of numbers\n",
    "# predicted is an array of numbers\n",
    "# return the number of correct predictions/total\n",
    "def error_perc(actual, predicted):\n",
    "    correct = 0;\n",
    "    for x, y in zip(actual, predicted):\n",
    "        for i in x:\n",
    "            if y == i:\n",
    "                correct = correct + 1\n",
    "                # print(x, y)\n",
    "                # print(str(correct) + \", \" + str(len(actual)))\n",
    "    return correct / len(actual)\n",
    "\n",
    "ep = error_perc(y_test, test_preds)\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff9fcc-60b1-4458-8d79-5f012c7cec2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
